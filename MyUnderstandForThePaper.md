# GNN模型想法

论文中用的是MAE，可以增加几个。**MAE** 和 **RMSE** 一起，可以看出样本误差的离散程度。**MAE** 和 **MAPE** 一起，估算模型对不同数量级样例的拟合程度。**R²** 用于评估模型的解释能力。

残差连接可以改一改看看，密连接或其他。

研究一下GATv2

尝试加入Attention了，速度可能会变快，但是性能如何暂时未知。

六个数据集：

- Bulk crystals: 10.29%
- Alloy surfaces: 3.61%
- **MOFs: -9.66%**
- **2D materials: 0.05%**
- Pt clusters: 6.21%



# 1 评价指标

1. 平均绝对误差 (MAE)

   这个指标是对绝对误差损失的预期值。

$$
\text{MAE}(y, \hat{y}) = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i|
$$

2. 平均绝对百分比误差 (MAPE)

   这个指标是对相对误差损失的预期值。所谓相对误差，就是绝对误差和真值的百分比。

$$
\text{MAPE}(y, \hat{y}) = \frac{1}{n} \sum_{i=1}^{n} \frac{\|y_i - \hat{y}_i\|}{\|y_i\|}
$$

3. 均方误差 (MSE)

   该指标对应于平方（二次）误差的期望。

$$
\text{MSE}(y, \hat{y}) = \frac{1}{n} \sum_{i=1}^{n} \|y_i - \hat{y}_i\|_2^2
$$

4. 均方误差根或均方根误差 (RMSE)

   该指标对应于平方（二次）误差的期望。

$$
\text{RMSE}(y, \hat{y}) = \sqrt{\frac{1}{n} \sum_{i=1}^{n} \|y_i - \hat{y}_i\|_2^2}
$$

5. 均方误差对数 (MSLE, Mean Squared Log Error)该指标对应平方对数(二次)差的预期。

$$
\text{MSLE}(y, \hat{y}) = \frac{1}{n} \sum_{i=0}^{n} (\log(1 + y_i) - \log(1 + \hat{y}_i))^2
$$

6. 中位绝对误差 (MedAE, Median Absolute Error)通过取目标和预测之间的所有绝对差值的中值来计算损失。

$$
\text{MedAE}(y, \hat{y}) = \text{median}(| y_1 - \hat{y}_1 |, \ldots, | y_n - \hat{y}_n |)
$$

7. R Squared (r2 score)

   R Squared又叫可决系数(coefficient of determination)也叫拟合优度，反映的是自变量x对因变量y的变动的解释的程度。越接近于1，说明模型拟合得越好。

$$
R^2(y, \hat{y}) = 1 - \frac{\sum_{i=0}^{n} (y_i - \hat{y}_i)^2}{\sum_{i=0}^{n} (y_i - \bar{y})^2} = \frac{\text{ESS}}{\text{TSS}} = 1 - \frac{\text{RSS}}{\text{TSS}}
$$

其中，

$$
\text{TSS}(\text{Total Sum of Squares}) = \sum_{i=0}^{n} (y_i - \bar{y})^2
$$

表示的是y的变动的程度，正比于方差。

$$
\text{RSS}(\text{Residual Sum of Squares}) = \sum_{i=0}^{n} (y_i - \hat{y}_i)^2
$$

表示的是模型和真实值的残差。

$$
\text{ESS}(\text{Explained Sum of Squares}) = \sum_{i=0}^{n} (\hat{y}_i - \bar{y})^2
$$

表示的是模型对y的变动的预测。





# 2 指标选择（摘自其他博主的经验）

**单个指标使用的情况**

- MAE和MedAE基于绝对误差。如果看重真实值和预测值的绝对误差，则选用MAE或MedAE，两者分别是误差的均值和中位数。MAE对极端值比较敏感。
- 如果看重真实值和预测值的差的平方，则选用MSE或RMSE。
- 如果存在不同样本的真实值有量级差或者更加关注预测和真实值的百分比差异的情况，最好选用MAPE。
- 如果y具有随着x进行指数变动的趋势时，适合用MSLE。recall that $$\log(1 + x)$$ 是x的等阶无穷小和对x的对数化处理。
- 如果模型希望的是找到能够解释目标y变动的因变量，则选用R Squared更合适。

**多个指标配合使用的情况**

- MAE和RMSE一起使用，可以看出样本误差的离散程度。比如RMSE远大于MAE时，可以得知不同样例的误差差别很大。
- MAE和MAPE一起使用，再结合 $$\bar{y}$$，可以估算模型对不同数量级样例的拟合程度。比如MAE远大于 $$MAPE * \bar{y}$$ 则可能是模型对真实值小的样本预测更准。可以考虑为不同数量级的样本建立不同的模型。

**交叉检验中使用各种评价指标**

1. 不论是回归模型还是分类模型，机器学习要达到的都是好的泛化能力。而泛化能力是由测试误差估计的泛化误差来评判的。其中测试误差是测试集的预测值和真实值的统计量，也就是以上谈到的各种指标。

2. 设 $$\mu$$ 为测试误差的均值，$$\sigma$$ 为测试误差的标准差，$$\alpha$$ 为显著性水平，一般是5%。假设测试误差和泛化误差服从正态分布，置信度95%的区间为
   $$
   (\mu - Z_{0.025} * \sigma, \mu + Z_{0.025} * \sigma)
   $$

3. 不仅要关注各个指标的均值，也要关注方差，因为方差反映了模型泛化能力随着训练集改变的程度。



# 3 评价指标增加

**MAE** 和 **RMSE** 一起，可以看出样本误差的离散程度。**MAE** 和 **MAPE** 一起，估算模型对不同数量级样例的拟合程度。**R²** 用于评估模型的解释能力。









# 1 论文笔记

## 1.1 背景相关

1. **研究背景**：

   介绍了材料科学领域面临的挑战，即在化学设计空间中寻找新型高性能材料。现有的无机晶体材料数据库（如ICSD）规模有限，**无法满足**对新材料的需求。（需要设计新材料）

   AI和ML在材料发现中的重要性，尤其是在发现超长寿命电池、高效太阳能电池板和室温超导体等新型材料方面的潜力。

   

2. **研究方法**：

   重点介绍了基于深度学习（DL）的生成设计范式，通过深度生成模型学习原子组装规则，生成假设性材料结构或组成。

   强调了快速且准确的材料性质预测模型的重要性，用于从大量候选材料中筛选出最有潜力的材料，以供进一步的理论计算或实验验证。

   

3. **机器学习模型的发展**：

   比较了基于组成和基于结构的机器学习模型，指出基于结构的模型通常具有更高的预测准确性，但受限于数据稀缺性。

   介绍了图神经网络（GNN）在材料性质预测中的应用，并指出尽管GNN取得了显著进展，但现有模型的深度仍然有限，与计算机视觉和自然语言处理等领域相比差距较大。

   

4. **研究动机和目标**：

   提出了通过增加网络深度来提升材料性质预测性能的假设，并指出当前GNN模型的深度不足可能是性能提升的瓶颈。

   引入了残差跳跃连接和可微分组归一化（DGN）等技术，以解决深度GNN训练中的问题。

   

5. **研究贡献**：

   提出了DeeperGATGNN模型，这是一种基于图注意力机制的深度GNN，通过DGN和跳跃连接实现了非常深的网络架构。

   展示了DeeperGATGNN在多个基准数据集上的性能，证明其在大规模材料性质预测中的有效性和可扩展性。

   提出了将这种深度扩展策略应用于其他GNN架构的可能性，以进一步提升材料性质预测的性能。



## 1.2 模型结构

DeeperGATGNN模型架构

1. **AGAT层（Augmented Graph Attention Layers）**：

   使用增强图注意力（AGAT）机制，通过将节点特征向量与其连接边的特征相结合来提取局部依赖特征。

   仅用于提取邻近原子之间的局部依赖特征。

   

2. **局部软注意力（Local Soft-Attention）**：

   局部软注意力 $α_{ij}$ 表示节点 $i$ 和其邻居 $j$ 之间的权重系数，反映了节点 $j$ 对节点 $i$ 的重要性。

   公式如下：
   $$
   \alpha_{ij} = \frac{\exp(a_{ij})}{\sum_{k \in N_i} \exp(a_{ik})}
   $$
   其中，$N_i$ 表示节点 i 的邻域，$a_{ij}$ 是节点 $i$ 和 $j$ 之间的参数化权重系数。

   

3. 全局注意力 (Global Attention): - 全局注意力 $ g_i $ 在全局池化之前应用，计算每个节点的整体重要性。

   公式如下： 
   $$
   g_i = \frac{(\mathbf{x}_i \parallel \mathbf{E}) \cdot \mathbf{W}}{\sum_{\mathbf{x}_c \in \mathbf{x}} (\mathbf{x}_c \parallel \mathbf{E}) \cdot \mathbf{W}} 
   $$
   其中，$\mathbf{x} \in \mathbb{R}^F$ 表示学习到的嵌入，$\mathbf{E}$ 是晶体的组成向量，$\mathbf{W} \in \mathbb{R}^{1 \times (F + |E|)}$ 是参数化矩阵。

   

4. **DGN（Differentiable Group Normalization）**：

   DGN层用于进一步改进学习，通过在注意力层之间添加可微分组归一化来增强模型的深度学习能力。

   

5. **跳跃连接（Skip-Connections）**：

   在注意力层之间添加跳跃连接，以提取局部特征并进一步改进学习。

   

6. **全局池化层（Global Pooling Layer）**：

   在全局注意力层之后应用全局池化层，以整合全局信息。

   

7. **全连接隐藏层和输出层**：

   在全局池化层之后，添加几个全连接隐藏层，最后通过一个全连接层产生预测输出。

   

## 1.3 DeeperGATGNN模型的深度扩展

1. **深度扩展策略**：

   DeeperGATGNN通过在AGAT层之间添加跳跃连接和DGN层来实现深度扩展，从而克服过平滑问题，使模型能够更有效地提取晶体的物理依赖特征。

2. **模型性能**：

   通过增加图卷积（GC）层的数量，DeeperGATGNN能够在多个基准数据集上实现显著的性能提升，特别是在材料性质预测方面。

3. **可扩展性**：

   DeeperGATGNN展示了在材料性质预测中，与其他现有GNN模型相比，具有更高的可扩展性，尽管其训练参数数量相对较少。



